---
layout: post
title: The ghost of Christmas present
date: '2008-01-29T00:00:00+00:00'
permalink: the-ghost-of-christmas-present
---
<div class="pic"><a href="http://www.bic.org.uk/index.html"><img alt="bic.jpg" src="https://s3-eu-west-1.amazonaws.com/bibliocloudimages/snowblog/bic.jpg" width="91" height="80" /></a></div>

Rob's explained below about the ghost of Christmas past: here's one from the present day. I've just received word from Nielsen, defender of bibliographic truth and knight of ONIX data, that the reason some of our book records are not getting through to retailers (which I only discovered because Susie bothered to check for me - naively, I would have assumed that if I sent an ONIX compliant record to Nielsen, to whom I pay 400 a year, they would process it in a timely fashion and pass it on) is because Nielsen have not yet migrated to BIC 2.0. 

I know - another boring post from Emma about dry old data. Let me explain, though. BIC, the people who are responsible for providing classification codes for books, issued an updated classification system in April 2006. *2006*, note. This new system, 2.0, replaced the former one, 1.1.    

It has been 22 months since the old system was declared obsolete. 

I understand that many organisations will not have upgraded to 2.0, for all manner of reasons. However, as guardians of data and leaders in the field, Nielsen should be setting an example to everyone and encouraging them - through ceasing provision of 1.1 data after a suitable cross-over period of, say, 12 months - to migrate. Nielsen are able to insist on a whole bunch of strict requirements to process ONIX data, and they managed the ISBN13 migration, so they should be able to insist on BIC 2.0. 

To get round their tardiness they employ people to take my pristine ONIX data (the data I thoroughly validate to make sure there are no errors), to look at the BIC 2.0 code and to take a stab at deciding which BIC 1.1 code should be used instead. Human intervention, in my experience, always leads to error. That's why we've gone to the bother of implementing ONIX - so that there are no rekeying errors. Moreover, there's no straightforward correlation between the two classification systems so there is a real degree of subjectivity involved. There are precedents for their lack of accuracy: for instance, when I registered A A Milne's The Sunny Side with them, via ONIX data, they rekeyed it to be a children's book, thinking they knew better, causing untold lost sales as the witty, adult articles from Punch Magazine sat next to Disney editions of Winnie the Pooh stories. 

It matters which subject area a book is classified under - if it's a thriller but is classified as crime that's a completely different buyer, a completely different part of the shop, a completely different search on Amazon etc. And it matters that data management is as efficient as possible. So, I go to great lengths to make sure that our data is accurate and timely and machine readable. I am thus very disappointed that despite my best efforts, a team of people get a chance to fiddle around with it, delay its dissemination and, if they hit the jackpot, render it incorrect. <br/>Emma
